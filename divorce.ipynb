{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview and format data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q1</th>\n",
       "      <th>Q2</th>\n",
       "      <th>Q3</th>\n",
       "      <th>Q4</th>\n",
       "      <th>Q5</th>\n",
       "      <th>Q6</th>\n",
       "      <th>Q7</th>\n",
       "      <th>Q8</th>\n",
       "      <th>Q9</th>\n",
       "      <th>Q10</th>\n",
       "      <th>...</th>\n",
       "      <th>Q46</th>\n",
       "      <th>Q47</th>\n",
       "      <th>Q48</th>\n",
       "      <th>Q49</th>\n",
       "      <th>Q50</th>\n",
       "      <th>Q51</th>\n",
       "      <th>Q52</th>\n",
       "      <th>Q53</th>\n",
       "      <th>Q54</th>\n",
       "      <th>Divorce</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>170 rows Ã— 55 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Q1  Q2  Q3  Q4  Q5  Q6  Q7  Q8  Q9  Q10  ...  Q46  Q47  Q48  Q49  Q50  \\\n",
       "0     2   2   4   1   0   0   0   0   0    0  ...    2    1    3    3    3   \n",
       "1     4   4   4   4   4   0   0   4   4    4  ...    2    2    3    4    4   \n",
       "2     2   2   2   2   1   3   2   1   1    2  ...    3    2    3    1    1   \n",
       "3     3   2   3   2   3   3   3   3   3    3  ...    2    2    3    3    3   \n",
       "4     2   2   1   1   1   1   0   0   0    0  ...    2    1    2    3    2   \n",
       "..   ..  ..  ..  ..  ..  ..  ..  ..  ..  ...  ...  ...  ...  ...  ...  ...   \n",
       "165   0   0   0   0   0   0   0   0   0    0  ...    1    0    4    1    1   \n",
       "166   0   0   0   0   0   0   0   0   0    0  ...    4    1    2    2    2   \n",
       "167   1   1   0   0   0   0   0   0   0    1  ...    3    0    2    0    1   \n",
       "168   0   0   0   0   0   0   0   0   0    0  ...    3    3    2    2    3   \n",
       "169   0   0   0   0   0   0   0   1   0    0  ...    3    4    4    0    1   \n",
       "\n",
       "     Q51  Q52  Q53  Q54  Divorce  \n",
       "0      2    3    2    1        1  \n",
       "1      4    4    2    2        1  \n",
       "2      1    2    2    2        1  \n",
       "3      3    2    2    2        1  \n",
       "4      2    2    1    0        1  \n",
       "..   ...  ...  ...  ...      ...  \n",
       "165    4    2    2    2        0  \n",
       "166    2    3    2    2        0  \n",
       "167    1    3    0    0        0  \n",
       "168    2    4    3    1        0  \n",
       "169    3    3    3    1        0  \n",
       "\n",
       "[170 rows x 55 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Import data\n",
    "df = pd.read_csv('divorce_data.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Q1         int64\n",
       "Q2         int64\n",
       "Q3         int64\n",
       "Q4         int64\n",
       "Q5         int64\n",
       "Q6         int64\n",
       "Q7         int64\n",
       "Q8         int64\n",
       "Q9         int64\n",
       "Q10        int64\n",
       "Q11        int64\n",
       "Q12        int64\n",
       "Q13        int64\n",
       "Q14        int64\n",
       "Q15        int64\n",
       "Q16        int64\n",
       "Q17        int64\n",
       "Q18        int64\n",
       "Q19        int64\n",
       "Q20        int64\n",
       "Q21        int64\n",
       "Q22        int64\n",
       "Q23        int64\n",
       "Q24        int64\n",
       "Q25        int64\n",
       "Q26        int64\n",
       "Q27        int64\n",
       "Q28        int64\n",
       "Q29        int64\n",
       "Q30        int64\n",
       "Q31        int64\n",
       "Q32        int64\n",
       "Q33        int64\n",
       "Q34        int64\n",
       "Q35        int64\n",
       "Q36        int64\n",
       "Q37        int64\n",
       "Q38        int64\n",
       "Q39        int64\n",
       "Q40        int64\n",
       "Q41        int64\n",
       "Q42        int64\n",
       "Q43        int64\n",
       "Q44        int64\n",
       "Q45        int64\n",
       "Q46        int64\n",
       "Q47        int64\n",
       "Q48        int64\n",
       "Q49        int64\n",
       "Q50        int64\n",
       "Q51        int64\n",
       "Q52        int64\n",
       "Q53        int64\n",
       "Q54        int64\n",
       "Divorce    int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check type of data in each column\n",
    "\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Q1         0\n",
       "Q2         0\n",
       "Q3         0\n",
       "Q4         0\n",
       "Q5         0\n",
       "Q6         0\n",
       "Q7         0\n",
       "Q8         0\n",
       "Q9         0\n",
       "Q10        0\n",
       "Q11        0\n",
       "Q12        0\n",
       "Q13        0\n",
       "Q14        0\n",
       "Q15        0\n",
       "Q16        0\n",
       "Q17        0\n",
       "Q18        0\n",
       "Q19        0\n",
       "Q20        0\n",
       "Q21        0\n",
       "Q22        0\n",
       "Q23        0\n",
       "Q24        0\n",
       "Q25        0\n",
       "Q26        0\n",
       "Q27        0\n",
       "Q28        0\n",
       "Q29        0\n",
       "Q30        0\n",
       "Q31        0\n",
       "Q32        0\n",
       "Q33        0\n",
       "Q34        0\n",
       "Q35        0\n",
       "Q36        0\n",
       "Q37        0\n",
       "Q38        0\n",
       "Q39        0\n",
       "Q40        0\n",
       "Q41        0\n",
       "Q42        0\n",
       "Q43        0\n",
       "Q44        0\n",
       "Q45        0\n",
       "Q46        0\n",
       "Q47        0\n",
       "Q48        0\n",
       "Q49        0\n",
       "Q50        0\n",
       "Q51        0\n",
       "Q52        0\n",
       "Q53        0\n",
       "Q54        0\n",
       "Divorce    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check each column has null value or not?\n",
    "\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Q1', 'Q2', 'Q3', 'Q4', 'Q5', 'Q6', 'Q7', 'Q8', 'Q9', 'Q10', 'Q11',\n",
       "       'Q12', 'Q13', 'Q14', 'Q15', 'Q16', 'Q17', 'Q18', 'Q19', 'Q20', 'Q21',\n",
       "       'Q22', 'Q23', 'Q24', 'Q25', 'Q26', 'Q27', 'Q28', 'Q29', 'Q30', 'Q31',\n",
       "       'Q32', 'Q33', 'Q34', 'Q35', 'Q36', 'Q37', 'Q38', 'Q39', 'Q40', 'Q41',\n",
       "       'Q42', 'Q43', 'Q44', 'Q45', 'Q46', 'Q47', 'Q48', 'Q49', 'Q50', 'Q51',\n",
       "       'Q52', 'Q53', 'Q54', 'Divorce'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create suitable model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of not_divorce : 86\n",
      "Amount of divorce : 84\n"
     ]
    }
   ],
   "source": [
    "X = df.drop(['Divorce'],axis=1)\n",
    "y = df[['Divorce']]\n",
    "\n",
    "not_divorce=len(y[y['Divorce']==0])\n",
    "divorce=len(y[y['Divorce']==1])\n",
    "\n",
    "print('Amount of not_divorce : {}'.format(not_divorce))\n",
    "print('Amount of divorce : {}'.format(divorce))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above , My data is balance data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR   -> Accuracy : 98.235 % (Std : 5.294 %)\n",
      "kNN  -> Accuracy : 97.647 % (Std : 5.391 %)\n",
      "SVC  -> Accuracy : 97.647 % (Std : 5.391 %)\n",
      "RFC  -> Accuracy : 97.647 % (Std : 5.391 %)\n",
      "DT   -> Accuracy : 95.882 % (Std : 5.912 %)\n",
      "MLP  -> Accuracy : 97.059 % (Std : 5.423 %)\n",
      "GNB  -> Accuracy : 97.059 % (Std : 6.028 %)\n",
      "XGB  -> Accuracy : 97.647 % (Std : 5.391 %)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Candidated algorithm models\n",
    "models = []\n",
    "models.append(('LR', LogisticRegression(max_iter=10000)))\n",
    "models.append(('kNN', KNeighborsClassifier()))\n",
    "models.append(('SVC', SVC()))\n",
    "models.append(('RFC', RandomForestClassifier(random_state=0)))\n",
    "models.append(('DT', DecisionTreeClassifier(random_state=0)))\n",
    "models.append(('MLP', MLPClassifier( hidden_layer_sizes=32, max_iter=1000, random_state=0)))\n",
    "models.append(('GNB', GaussianNB()))\n",
    "models.append(('XGB', XGBClassifier(random_state=0)))\n",
    "\n",
    "names = []\n",
    "results = []\n",
    "\n",
    "for name, model in models:\n",
    "    scores = cross_val_score(model, X, y.values.ravel(), cv=10, scoring='accuracy')\n",
    "    names.append(name)\n",
    "    results.append(scores)\n",
    "    print('{:4} -> Accuracy : {:.3f} % (Std : {:.3f} %)'.format(name, scores.mean()*100, scores.std()*100))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above , I choose Logistic regression because it has maximum accuracy and minimum std among others\n",
    "\n",
    "Next step -> Tuning process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 576 candidates, totalling 5760 fits\n",
      "Best hyperparameters: {'dual': False, 'fit_intercept': True, 'max_iter': 10000, 'multi_class': 'auto', 'penalty': 'l2', 'solver': 'lbfgs', 'warm_start': True}\n",
      "0.9823529411764707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Panupong Jindarat\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
      "4280 fits failed out of a total of 5760.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "240 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Panupong Jindarat\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Panupong Jindarat\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Panupong Jindarat\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "80 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Panupong Jindarat\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Panupong Jindarat\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1216, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Panupong Jindarat\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Panupong Jindarat\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "240 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Panupong Jindarat\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Panupong Jindarat\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Panupong Jindarat\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "240 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Panupong Jindarat\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Panupong Jindarat\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Panupong Jindarat\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cholesky supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "240 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Panupong Jindarat\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Panupong Jindarat\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Panupong Jindarat\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "480 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Panupong Jindarat\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Panupong Jindarat\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Panupong Jindarat\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 59, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver saga supports only dual=False, got dual=True\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "240 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Panupong Jindarat\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Panupong Jindarat\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Panupong Jindarat\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 59, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only dual=False, got dual=True\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "240 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Panupong Jindarat\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Panupong Jindarat\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Panupong Jindarat\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 59, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only dual=False, got dual=True\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "240 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Panupong Jindarat\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Panupong Jindarat\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Panupong Jindarat\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 59, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cholesky supports only dual=False, got dual=True\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "240 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Panupong Jindarat\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Panupong Jindarat\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Panupong Jindarat\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 59, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only dual=False, got dual=True\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "240 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Panupong Jindarat\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Panupong Jindarat\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Panupong Jindarat\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "240 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Panupong Jindarat\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Panupong Jindarat\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Panupong Jindarat\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 64, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "240 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Panupong Jindarat\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Panupong Jindarat\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Panupong Jindarat\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "240 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Panupong Jindarat\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Panupong Jindarat\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Panupong Jindarat\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cholesky supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "240 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Panupong Jindarat\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Panupong Jindarat\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Panupong Jindarat\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "80 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Panupong Jindarat\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Panupong Jindarat\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1216, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Panupong Jindarat\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Panupong Jindarat\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=True\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "240 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Panupong Jindarat\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Panupong Jindarat\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1207, in fit\n",
      "    multi_class = _check_multi_class(self.multi_class, solver, len(self.classes_))\n",
      "  File \"c:\\Users\\Panupong Jindarat\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 90, in _check_multi_class\n",
      "    raise ValueError(\"Solver %s does not support a multinomial backend.\" % solver)\n",
      "ValueError: Solver liblinear does not support a multinomial backend.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "120 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Panupong Jindarat\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Panupong Jindarat\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1291, in fit\n",
      "    fold_coefs_ = Parallel(n_jobs=self.n_jobs, verbose=self.verbose, prefer=prefer)(\n",
      "  File \"c:\\Users\\Panupong Jindarat\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py\", line 1085, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"c:\\Users\\Panupong Jindarat\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py\", line 901, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"c:\\Users\\Panupong Jindarat\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py\", line 819, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"c:\\Users\\Panupong Jindarat\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"c:\\Users\\Panupong Jindarat\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 597, in __init__\n",
      "    self.results = batch()\n",
      "  File \"c:\\Users\\Panupong Jindarat\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py\", line 288, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"c:\\Users\\Panupong Jindarat\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py\", line 288, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"c:\\Users\\Panupong Jindarat\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\fixes.py\", line 117, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"c:\\Users\\Panupong Jindarat\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 521, in _logistic_regression_path\n",
      "    alpha = (1.0 / C) * (1 - l1_ratio)\n",
      "TypeError: unsupported operand type(s) for -: 'int' and 'NoneType'\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "80 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Panupong Jindarat\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Panupong Jindarat\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1216, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"c:\\Users\\Panupong Jindarat\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1223, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"c:\\Users\\Panupong Jindarat\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1062, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "80 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Panupong Jindarat\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Panupong Jindarat\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1207, in fit\n",
      "    multi_class = _check_multi_class(self.multi_class, solver, len(self.classes_))\n",
      "  File \"c:\\Users\\Panupong Jindarat\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 90, in _check_multi_class\n",
      "    raise ValueError(\"Solver %s does not support a multinomial backend.\" % solver)\n",
      "ValueError: Solver newton-cholesky does not support a multinomial backend.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\Panupong Jindarat\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_search.py:953: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan 0.97647059 0.97647059        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan 0.97647059 0.97647059        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan 0.97058824 0.97058824        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan 0.97058824 0.97058824        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan 0.97647059 0.97647059        nan        nan\n",
      "        nan        nan        nan        nan 0.97647059 0.97647059\n",
      " 0.98235294 0.98235294 0.97647059 0.97647059 0.98235294 0.98235294\n",
      " 0.98235294 0.98235294 0.97647059 0.97647059 0.97647059 0.97647059\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.97647059 0.97647059        nan        nan 0.97647059 0.97647059\n",
      " 0.96470588 0.96470588 0.97647059 0.97647059 0.97647059 0.97647059\n",
      "        nan        nan 0.97647059 0.97647059        nan        nan\n",
      "        nan        nan        nan        nan 0.97647059 0.97647059\n",
      " 0.98235294 0.98235294 0.97647059 0.97647059 0.98235294 0.98235294\n",
      " 0.98235294 0.98235294 0.97647059 0.97647059 0.97647059 0.97647059\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.97647059 0.97647059        nan        nan 0.97647059 0.97647059\n",
      " 0.96470588 0.96470588 0.97647059 0.97647059 0.97647059 0.97647059\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan 0.97058824 0.97058824\n",
      " 0.98235294 0.98235294        nan        nan 0.98235294 0.98235294\n",
      "        nan        nan 0.97647059 0.97647059 0.97647059 0.97647059\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.97647059 0.97647059        nan        nan 0.97647059 0.97647059\n",
      "        nan        nan 0.97647059 0.97647059 0.97647059 0.97647059\n",
      "        nan        nan 0.95294118 0.94705882        nan        nan\n",
      "        nan        nan        nan        nan 0.95294118 0.95294118\n",
      " 0.97058824 0.97058824 0.97058824 0.97058824 0.97058824 0.97058824\n",
      " 0.97058824 0.97058824 0.97058824 0.97058824 0.97058824 0.97058824\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.97647059 0.97647059        nan        nan 0.97647059 0.97647059\n",
      " 0.93529412 0.93529412 0.97058824 0.97058824 0.97058824 0.97058824\n",
      "        nan        nan 0.95294118 0.94705882        nan        nan\n",
      "        nan        nan        nan        nan 0.95294118 0.95294118\n",
      " 0.97058824 0.97058824 0.97058824 0.97058824 0.97058824 0.97058824\n",
      " 0.97058824 0.97058824 0.97058824 0.97058824 0.97058824 0.97058824\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.97647059 0.97647059        nan        nan 0.97647059 0.97647059\n",
      " 0.93529412 0.93529412 0.97058824 0.97058824 0.97058824 0.97058824\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan 0.95294118 0.95294118\n",
      " 0.97058824 0.97058824        nan        nan 0.97058824 0.97058824\n",
      "        nan        nan 0.97058824 0.97058824 0.97058824 0.97058824\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.97647059 0.97647059        nan        nan 0.97647059 0.97647059\n",
      "        nan        nan 0.97058824 0.97058824 0.97058824 0.97058824]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the range of hyperparameters to optimize\n",
    "param_grid = {'penalty': ['l1', 'l2', 'elasticnet', None],\n",
    "                'dual': [True, False],\n",
    "                'fit_intercept': [True, False],\n",
    "                'solver': ['lbfgs', 'liblinear', 'newton-cg','newton-cholesky', 'sag', 'saga'],\n",
    "                'multi_class': ['auto', 'ovr', 'multinomial'],\n",
    "                'warm_start': [True, False],\n",
    "                'max_iter': [10000]\n",
    "            }\n",
    "\n",
    "# Perform the grid search\n",
    "grid_search = GridSearchCV(LogisticRegression(), \n",
    "                            param_grid, \n",
    "                            scoring='accuracy', \n",
    "                            cv=10, \n",
    "                            verbose=1) \n",
    "grid_search.fit(X, y.values.ravel())\n",
    "\n",
    "print(f\"Best hyperparameters: {grid_search.best_params_}\")\n",
    "print(grid_search.best_score_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above , I get the best model which has parameter like those \n",
    "\n",
    "Then , I check my tuning model is overfitting or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "LR =  LogisticRegression(**grid_search.best_params_)\n",
    "\n",
    "train_sizes, train_scores, test_scores = learning_curve(LR,\n",
    "                                                        X, y.values.ravel(), cv=10, train_sizes=np.linspace(.1, 1.0, 5),scoring='accuracy')\n",
    "\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.1, color=\"r\")\n",
    "plt.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n",
    "plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n",
    "plt.title('LR')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Sizes')\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from graph above , there is no large gap between these two lines from LR model \n",
    "\n",
    "So I choose it as the best model for divorce prediction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature importance and Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q40   : 0.596\n",
      "Q26   : 0.493\n",
      "Q49   : 0.458\n",
      "Q44   : 0.372\n",
      "Q31   : 0.367\n",
      "Q17   : 0.358\n",
      "Q3    : 0.351\n",
      "Q28   : 0.337\n",
      "Q6    : 0.328\n",
      "Q39   : 0.309\n",
      "Q20   : 0.306\n",
      "Q15   : 0.303\n",
      "Q53   : 0.302\n",
      "Q18   : 0.299\n",
      "Q2    : 0.283\n",
      "Q19   : 0.278\n",
      "Q1    : 0.268\n",
      "Q52   : 0.247\n",
      "Q41   : 0.225\n",
      "Q30   : 0.208\n",
      "Q38   : 0.207\n",
      "Q14   : 0.206\n",
      "Q36   : 0.205\n",
      "Q34   : 0.202\n",
      "Q33   : 0.196\n",
      "Q29   : 0.189\n",
      "Q32   : 0.182\n",
      "Q50   : 0.181\n",
      "Q42   : 0.161\n",
      "Q5    : 0.161\n",
      "Q12   : 0.152\n",
      "Q43   : 0.15 \n",
      "Q16   : 0.149\n",
      "Q25   : 0.148\n",
      "Q4    : 0.14 \n",
      "Q11   : 0.135\n",
      "Q27   : 0.116\n",
      "Q21   : 0.096\n",
      "Q9    : 0.093\n",
      "Q13   : 0.089\n",
      "Q8    : 0.077\n",
      "Q48   : 0.072\n",
      "Q24   : 0.069\n",
      "Q35   : 0.069\n",
      "Q47   : 0.066\n",
      "Q54   : 0.052\n",
      "Q46   : 0.047\n",
      "Q10   : 0.044\n",
      "Q23   : 0.034\n",
      "Q37   : 0.027\n",
      "Q22   : 0.025\n",
      "Q45   : 0.013\n",
      "Q51   : 0.005\n",
      "Q7    : 0.001\n"
     ]
    }
   ],
   "source": [
    "X = df.drop(['Divorce'],axis=1)\n",
    "y = df[['Divorce']]\n",
    "\n",
    "LR.fit(X,y.values.ravel())\n",
    "\n",
    "feature_importances = LR.coef_\n",
    "\n",
    "# calculate absolute feature importance scores across all classes\n",
    "feature_importances_abs = np.absolute(feature_importances[0])\n",
    "\n",
    "# create a dictionary of feature names and their corresponding absolute importance scores\n",
    "feature_scores = {X.columns[i] : score for i, score in enumerate(feature_importances_abs)}\n",
    "\n",
    "# sort feature names based on their scores\n",
    "sorted_features = sorted(feature_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# print the sorted feature names with their scores\n",
    "for feature, score in sorted_features:\n",
    "    print('{:<5} : {:<5}'.format(feature, score.round(3)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From result above , \n",
    "\n",
    "feature Q40 has the highest score of 0.596, which means that it has the greatest impact on the target variable. \n",
    "\n",
    "Similarly, Q26, Q49, Q44, and Q31 also have high scores, indicating that they are important features. \n",
    "\n",
    "On the other hand, features with near zero scores have little or no impact on the target variable.\n",
    "\n",
    "The next step -> try to keep top .... features and evaluate accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 3     -> Accuracy 99.412 % (Std 0.000 %)\n",
      "Top 10    -> Accuracy 99.412 % (Std 0.000 %)\n",
      "Top 11    -> Accuracy 99.412 % (Std 0.000 %)\n",
      "Top 2     -> Accuracy 98.824 % (Std 0.000 %)\n",
      "Top 4     -> Accuracy 98.824 % (Std 0.000 %)\n",
      "Top 5     -> Accuracy 98.824 % (Std 0.000 %)\n",
      "Top 7     -> Accuracy 98.824 % (Std 0.000 %)\n",
      "Top 8     -> Accuracy 98.824 % (Std 0.000 %)\n",
      "Top 9     -> Accuracy 98.824 % (Std 0.000 %)\n",
      "Top 12    -> Accuracy 98.824 % (Std 0.000 %)\n",
      "Top 13    -> Accuracy 98.824 % (Std 0.000 %)\n",
      "Top 14    -> Accuracy 98.824 % (Std 0.000 %)\n",
      "Top 15    -> Accuracy 98.824 % (Std 0.000 %)\n",
      "Top 6     -> Accuracy 98.235 % (Std 0.000 %)\n",
      "Top 16    -> Accuracy 98.235 % (Std 0.000 %)\n",
      "Top 17    -> Accuracy 98.235 % (Std 0.000 %)\n",
      "Top 18    -> Accuracy 98.235 % (Std 0.000 %)\n",
      "Top 19    -> Accuracy 98.235 % (Std 0.000 %)\n",
      "Top 20    -> Accuracy 98.235 % (Std 0.000 %)\n",
      "Top 21    -> Accuracy 98.235 % (Std 0.000 %)\n",
      "Top 22    -> Accuracy 98.235 % (Std 0.000 %)\n",
      "Top 23    -> Accuracy 98.235 % (Std 0.000 %)\n",
      "Top 24    -> Accuracy 98.235 % (Std 0.000 %)\n",
      "Top 25    -> Accuracy 98.235 % (Std 0.000 %)\n",
      "Top 26    -> Accuracy 98.235 % (Std 0.000 %)\n",
      "Top 27    -> Accuracy 98.235 % (Std 0.000 %)\n",
      "Top 28    -> Accuracy 98.235 % (Std 0.000 %)\n",
      "Top 29    -> Accuracy 98.235 % (Std 0.000 %)\n",
      "Top 30    -> Accuracy 98.235 % (Std 0.000 %)\n",
      "Top 31    -> Accuracy 98.235 % (Std 0.000 %)\n",
      "Top 32    -> Accuracy 98.235 % (Std 0.000 %)\n",
      "Top 33    -> Accuracy 98.235 % (Std 0.000 %)\n",
      "Top 34    -> Accuracy 98.235 % (Std 0.000 %)\n",
      "Top 35    -> Accuracy 98.235 % (Std 0.000 %)\n",
      "Top 36    -> Accuracy 98.235 % (Std 0.000 %)\n",
      "Top 37    -> Accuracy 98.235 % (Std 0.000 %)\n",
      "Top 38    -> Accuracy 98.235 % (Std 0.000 %)\n",
      "Top 39    -> Accuracy 98.235 % (Std 0.000 %)\n",
      "Top 40    -> Accuracy 98.235 % (Std 0.000 %)\n",
      "Top 41    -> Accuracy 98.235 % (Std 0.000 %)\n",
      "Top 42    -> Accuracy 98.235 % (Std 0.000 %)\n",
      "Top 43    -> Accuracy 98.235 % (Std 0.000 %)\n",
      "Top 44    -> Accuracy 98.235 % (Std 0.000 %)\n",
      "Top 45    -> Accuracy 98.235 % (Std 0.000 %)\n",
      "Top 46    -> Accuracy 98.235 % (Std 0.000 %)\n",
      "Top 47    -> Accuracy 98.235 % (Std 0.000 %)\n",
      "Top 48    -> Accuracy 98.235 % (Std 0.000 %)\n",
      "Top 49    -> Accuracy 98.235 % (Std 0.000 %)\n",
      "Top 50    -> Accuracy 98.235 % (Std 0.000 %)\n",
      "Top 51    -> Accuracy 98.235 % (Std 0.000 %)\n",
      "Top 52    -> Accuracy 98.235 % (Std 0.000 %)\n",
      "Top 53    -> Accuracy 98.235 % (Std 0.000 %)\n",
      "Top 54    -> Accuracy 98.235 % (Std 0.000 %)\n",
      "Top 1     -> Accuracy 97.059 % (Std 0.000 %)\n",
      "----------------------------------------\n",
      "['Q40', 'Q26', 'Q49']\n"
     ]
    }
   ],
   "source": [
    "# Create an empty list to store the accuracy and std of each set of selected features\n",
    "accuracy_std_list = []\n",
    "\n",
    "for i in range(len(sorted_features)):\n",
    "    col = []\n",
    "    for j in range(len(sorted_features[:i+1])):\n",
    "        col.append(sorted_features[j][0])\n",
    "\n",
    "    X = df[col]\n",
    "    y = df[['Divorce']]\n",
    "\n",
    "    scores = cross_val_score(LR, X, y.values.ravel(), cv=10, scoring='accuracy')\n",
    "    accuracy = scores.mean()\n",
    "    std = score.std()\n",
    "    accuracy_std_list.append((i+1, accuracy,std))\n",
    "\n",
    "# Sort the accuracy list in descending order based on accuracy\n",
    "accuracy_std_list = sorted(accuracy_std_list, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "for n_top, score , std in accuracy_std_list:\n",
    "    print('Top {:<5} -> Accuracy {:.3f} % (Std {:.3f} %)'.format(n_top, score*100,std*100))\n",
    "\n",
    "print('-'*40)\n",
    "\n",
    "top_features = []\n",
    "\n",
    "for feature_tuple in sorted_features[:accuracy_std_list[0][0]]:\n",
    "    top_features.append(feature_tuple[0])\n",
    "\n",
    "# Print the top feature names\n",
    "print(top_features)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From result above , I select ['Q40', 'Q26', 'Q49'] as features to predict divorce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        14\n",
      "           1       1.00      1.00      1.00        20\n",
      "\n",
      "    accuracy                           1.00        34\n",
      "   macro avg       1.00      1.00      1.00        34\n",
      "weighted avg       1.00      1.00      1.00        34\n",
      "\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# top_features = ['Q40', 'Q26', 'Q49']\n",
    "\n",
    "X = df[top_features]\n",
    "y = df[['Divorce']]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y.values.ravel(), test_size=0.2, random_state=42)\n",
    "\n",
    "### Predict Divorce ###\n",
    "\n",
    "LR = LogisticRegression(**grid_search.best_params_)\n",
    "\n",
    "LR.fit(X_train, y_train)\n",
    "\n",
    "y_pred_proba = LR.predict_proba(X_test)[:, 1]  # predicted probabilities of positive class\n",
    "\n",
    "y_pred = (y_pred_proba >= 0.5).astype(int)  # predicted class labels\n",
    "cm = confusion_matrix(y_test, y_pred)  # confusion matrix\n",
    "\n",
    "sns.heatmap(cm, annot=True, cmap='coolwarm')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "print('-'*40)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fbf4a95fc2be70f1a444ecbcde922f3dc928269856016871352e455d394062e8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
